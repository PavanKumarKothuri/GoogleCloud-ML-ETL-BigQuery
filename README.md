# **GCP Data Pipeline for ML - ETL on BigQuery** ğŸ—ï¸ğŸš€  

ğŸ“Œ **Tech Stack:** `Python` | `Pandas` | `Google Cloud Storage (GCS)` | `BigQuery` | `gCloud CLI`  

ğŸ“Œ **Business Goal:** Build a **scalable data pipeline** to clean, transform, and analyze large datasets for **Machine Learning and Business Intelligence**.  

ğŸ“Œ **Problem Solved:** Companies deal with **raw, messy data** that needs to be **cleaned, processed, and stored efficiently** before ML models can use it. This project automates the **Extract, Transform, Load (ETL) process** using **GCP services** and **Python**.  

---

## ğŸš€ **Project Overview**  
This project automates **data ingestion, transformation, and storage** using **Google Cloud** services. The ETL pipeline extracts raw data from **CSV files**, cleans it using **Pandas**, and loads it into **BigQuery** for analysis. The entire process is **automated using Python & gCloud CLI**.  

âœ… **Fully automated with Python & Cloud CLI**  
âœ… **Minimal manual setup** for efficiency  
âœ… **Scalable, cost-effective & secure**  
âœ… **Supports large datasets without performance issues**  

---

## âš¡ **Project Architecture**  

ğŸ”¹ **Extract**: Raw CSV data is uploaded to **Google Cloud Storage (GCS)**.  
ğŸ”¹ **Transform**: Data cleaning, type conversion, and feature engineering using **Pandas**.  
ğŸ”¹ **Load**: The cleaned data is loaded into **BigQuery** for SQL-based querying.  

ğŸ“Œ **Workflow:**  
1ï¸âƒ£ Upload raw data to **Cloud Storage** ğŸ“¤  
2ï¸âƒ£ Python script cleans and processes the data ğŸ§¹  
3ï¸âƒ£ Processed data is loaded into **BigQuery** ğŸ¯  
4ï¸âƒ£ Run SQL queries to analyze insights ğŸ“Š  

---

## ğŸ›  **Technical Implementation**  

### **1ï¸âƒ£ File Structure** ğŸ“‚  
```
gcp-etl/
â”‚â”€â”€ config.py               # GCP project & bucket details  
â”‚â”€â”€ upload_to_gcs.py        # Upload raw data to Cloud Storage  
â”‚â”€â”€ transform_data.py       # Clean & process data using Pandas  
â”‚â”€â”€ load_to_bigquery.py     # Load transformed data into BigQuery  
â”‚â”€â”€ query_bigquery.py       # Run SQL queries on BigQuery  
â”‚â”€â”€ requirements.txt        # Python dependencies  
â”‚â”€â”€ sample_data.csv         # Sample dataset  
â”‚â”€â”€ README.md               # Project documentation  
```

---

### **2ï¸âƒ£ Setup & Deployment** ğŸš€  

#### **ğŸ”¹ Step 1: Set up Google Cloud Project**  
```sh
gcloud auth login
gcloud config set project YOUR_PROJECT_ID
```

#### **ğŸ”¹ Step 2: Enable Required Services**  
```sh
gcloud services enable storage.googleapis.com
gcloud services enable bigquery.googleapis.com
```

#### **ğŸ”¹ Step 3: Install Dependencies & Activate Virtual Environment**  
```sh
python3 -m venv venv  
source venv/bin/activate  
pip install -r requirements.txt  
```

#### **ğŸ”¹ Step 4: Upload Data to Cloud Storage**  
```sh
python upload_to_gcs.py
```

#### **ğŸ”¹ Step 5: Transform Data using Pandas**  
```sh
python transform_data.py
```

#### **ğŸ”¹ Step 6: Load Data into BigQuery**  
```sh
python load_to_bigquery.py
```

#### **ğŸ”¹ Step 7: Query Data from BigQuery**  
```sh
python query_bigquery.py
```

---

## ğŸŒŸ **Challenges & Solutions**  

| Challenge ğŸ¤¯ | Solution âœ… |
|-------------|------------|
| Large datasets slowing down processing ğŸš€ | Used **BigQueryâ€™s distributed SQL engine** for fast queries. |
| Manual data uploads were inefficient â³ | Automated **Cloud Storage uploads** with `gcloud` and Python. |
| Schema mismatches when loading into BigQuery âŒ | Used **data type conversion & schema validation** in Pandas. |
| Cost optimization for BigQuery usage ğŸ’¸ | **Pay-per-query model** and **table partitioning** to reduce costs. |
| Security & Access Control ğŸ”’ | **IAM roles** and **Service Accounts** to restrict permissions. |

---

## ğŸ“ˆ **Business Impact**  

âœ… **Faster Decision Making**: Processed data is **immediately available** for business insights.  
âœ… **Scalability**: Works for **both small and large datasets** without infrastructure concerns.  
âœ… **Automation**: Reduces **manual errors** and **improves efficiency** in ETL workflows.  
âœ… **Cost-Efficient**: Pay-as-you-go model saves money compared to traditional databases.  

---

## ğŸ”¥ **Future Improvements**  

âœ… **Integrate Apache Airflow** for orchestration ğŸ¯  
âœ… **Implement real-time streaming** with **Pub/Sub & Dataflow** ğŸ“¡  
âœ… **Use BigQuery ML** for direct machine learning model training ğŸ¤–  
âœ… **Deploy on Google Cloud Functions** for serverless execution ğŸŒ  

---

## ğŸ¯ **Conclusion**  

This **GCP Data Pipeline** is a **powerful, scalable, and automated** ETL solution. It allows businesses to **process raw data efficiently** and **gain real-time insights** using BigQuery. The **automation** reduces human effort, and the **cloud-based architecture** ensures **high availability and security**.  

---

## â­ **Contribute & Connect**  
Built with â¤ï¸ by **PavanKumar Kothuri** â€“ ğŸš€ **Coding is Fun!** ğŸ˜ƒ  

- ğŸŒ [LinkedIn Profile](https://www.linkedin.com/in/Pavankumarkothuri/)
- ğŸ’» [GitHub Profile](https://github.com/PavanKumarKothuri)  
- âœ‰ï¸ [Email: pavankumarkothuri9@gmail.com](mailto:pavankumarkothuri9@gmail.com)

Feel free to connect with me for further discussions or contributions. ğŸŒŸ **Happy Coding!** ğŸš€